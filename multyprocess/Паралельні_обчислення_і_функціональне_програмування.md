
### **1. Вступ **

У сучасному програмуванні зростає потреба в ефективному опрацюванні великих обсягів даних. Послідовне виконання операцій, коли програма обробляє кожен елемент по черзі, стає серйозним обмеженням для масштабованих систем. Один із підходів до вирішення цієї проблеми — **паралельні обчислення**, що дозволяють розділити завдання на кілька частин і обробляти їх одночасно. Це особливо важливо для задач, що виконуються на багатоядерних процесорах або розподілених кластерах.

Розробка програм, що підтримують паралельне виконання, традиційно пов’язана з труднощами: необхідно враховувати взаємодію між потоками, контроль стану змінних і обробку конфліктів доступу до пам’яті. Одним із рішень цих проблем є **функціональне програмування**, оскільки воно мінімізує побічні ефекти та змінюваний стан, що робить його ідеальним для розпаралелювання.

Функціональне програмування базується на обробці даних через **чисті функції**, які не змінюють зовнішній стан і завжди повертають однаковий результат для однакових вхідних даних. Завдяки цьому такі функції можна легко виконувати паралельно, оскільки вони не залежать одна від одної і не створюють конфліктів при доступі до ресурсів.

Особливо важливими для паралельних обчислень є три ключові концепції функціонального програмування: **map**, **reduce** та **filter**. Вони дозволяють розподіляти обчислення між незалежними процесами або вузлами, використовуючи декларативний підхід. Функція **map** застосовує обчислення до кожного елемента колекції незалежно, **reduce** агрегує результати, а **filter** дозволяє відбирати необхідні дані. Завдяки цим операціям можна ефективно організувати паралельну обробку, уникнувши складнощів класичного потокового програмування.

Ці принципи стали основою для технології **MapReduce**, яка широко використовується у великих розподілених системах для аналізу та обробки великих даних. Вона дозволяє розділити великі завдання на підзадачі, розподілити їх між вузлами кластера та об’єднати результати в єдиний підсумок.

У цій лекції ми спочатку розглянемо функціональне програмування та його ключові концепції у контексті паралельних обчислень, потім перейдемо до глибшого аналізу технології MapReduce і розберемо її реалізацію на практичному прикладі.

### **2. Функції першого класу у функціональному програмуванні **

Функціональне програмування базується на кількох фундаментальних концепціях, і однією з найважливіших є **функції першого класу**. Це означає, що у програмній мові функції можна використовувати так само, як і будь-які інші змінні. Вони можуть передаватися як аргументи в інші функції, зберігатися у структурах даних або навіть повертатися як результат виконання іншої функції.

Розглянемо цей принцип на прикладі Python:

```python
def square(x):
    return x * x

print(square(5))  # Виведе: 25

# Присвоєння функції змінній
f = square
print(f(5))  # Виведе: 25

# Передача функції як аргумент
def apply_function(func, value):
    return func(value)

print(apply_function(square, 6))  # Виведе: 36
```

Тут ми бачимо, що функція `square` може бути присвоєна змінній `f` і викликана через неї. Крім того, ми можемо передати її як аргумент у функцію `apply_function`, яка отримує іншу функцію і застосовує її до значення.

Цей підхід відкриває можливість створювати більш гнучкі та універсальні функції. Наприклад, замість того, щоб писати окремі реалізації для різних операцій, ми можемо передавати функцію, яка визначає поведінку:

```python
def add_one(x):
    return x + 1

def multiply_by_two(x):
    return x * 2

operations = [add_one, multiply_by_two, square]

for op in operations:
    print(op(3))  # Виведе: 4, 6, 9
```

Ще одна важлива властивість функцій першого класу — можливість повертати функції як результат. Це особливо корисно для створення динамічних обчислювальних процесів:

```python
def make_multiplier(n):
    def multiplier(x):
        return x * n
    return multiplier

times_three = make_multiplier(3)
print(times_three(10))  # Виведе: 30
```

Це дозволяє створювати узагальнені функції, які можна легко адаптувати до конкретних завдань. Наприклад, замість того, щоб жорстко задавати множник, ми можемо створювати нові функції на льоту, передаючи потрібні параметри.

Функції першого класу є основою для **вищих функцій порядку**, таких як `map`, `reduce` і `filter`. Вони дозволяють маніпулювати колекціями даних декларативним способом, що робить код більш зрозумілим та ефективним, особливо в контексті паралельних обчислень. Наступним кроком ми розглянемо, як ці функції працюють і як вони використовуються у практичних задачах.

### **3. Основи функціонального програмування **

Функціональне програмування пропонує набір інструментів, які спрощують роботу з колекціями даних та дозволяють легко паралелізувати обчислення. Серед них особливе місце займають три ключові операції: **map**, **reduce** та **filter**. Ці функції широко застосовуються у паралельних обчисленнях, оскільки вони дозволяють виконувати обчислення без зміни стану програми та без взаємодії між потоками, що є критично важливим для ефективної роботи на багатоядерних системах і розподілених кластерах.

#### **Функція map**

Функція `map` виконує однакову операцію над кожним елементом колекції, не змінюючи її структури. Це дозволяє виконувати обчислення над кожним елементом незалежно, що ідеально підходить для паралельної обробки. В контексті розподілених обчислень це означає, що можна розділити набір даних на частини і обробляти їх на різних процесорах або серверах одночасно.

Розглянемо простий приклад: піднесення кожного елемента списку до квадрату.

```python
numbers = [1, 2, 3, 4, 5]

# Використання функції map
squared_numbers = list(map(lambda x: x ** 2, numbers))

print(squared_numbers)  # Виведе: [1, 4, 9, 16, 25]
```

Кожен елемент списку обробляється окремо, а результатом є новий список з обчисленими значеннями. Цей процес легко розпаралелюється, оскільки жодна з операцій не залежить від іншої.

Якщо потрібно виконати ці обчислення паралельно, можна використати бібліотеку `multiprocessing`, яка дозволяє обробляти дані в декількох процесах:

```python
from multiprocessing import Pool

def square(x):
    return x ** 2

with Pool() as pool:
    squared_numbers = pool.map(square, numbers)

print(squared_numbers)  # Виведе: [1, 4, 9, 16, 25]
```

Тут ми створюємо пул процесів і застосовуємо функцію `square` до кожного елемента списку одночасно, що значно пришвидшує обчислення на багатоядерних процесорах.

#### **Функція reduce**

Функція `reduce` використовується для агрегування результатів у єдине значення. Вона ідеально підходить для завдань, де потрібно обчислити суму, добуток, максимум або інші агреговані значення для великого набору даних.

Ось приклад використання `reduce` для підрахунку суми всіх елементів у списку:

```python
from functools import reduce

numbers = [1, 2, 3, 4, 5]

sum_of_numbers = reduce(lambda x, y: x + y, numbers)

print(sum_of_numbers)  # Виведе: 15
```

Ця операція поступово комбінує елементи списку: спочатку складає перші два, потім результат додає до наступного елемента і так далі.

```python
from functools import reduce


def fibonacci(count):
sequence = (0, 1)

  

for _ in range(2, count):
sequence += (reduce(lambda a, b: a + b, sequence[-2:]), )

  

return sequence[:count]

  

print(fibonacci(12))
```

У контексті паралельних обчислень `reduce` може бути розбитий на кілька етапів: спочатку обчислення виконується на різних підмножинах даних, а потім часткові результати об’єднуються в єдиний підсумок. Це є ключовою ідеєю функції **Reduce** у MapReduce.



#### **Функція filter**

Функція `filter` використовується для вибору елементів, які задовольняють певну умову. Це дуже корисно при роботі з великими наборами даних, коли потрібно відфільтрувати лише необхідні елементи перед їх подальшою обробкою.

Розглянемо приклад фільтрації парних чисел:

```python
numbers = [1, 2, 3, 4, 5, 6, 7, 8]

even_numbers = list(filter(lambda x: x % 2 == 0, numbers))

print(even_numbers)  # Виведе: [2, 4, 6, 8]
```

Як і `map`, функція `filter` може бути розпаралелена, якщо обробка кожного елемента не залежить від інших.

#### **Як ці функції працюють у паралельних обчисленнях?**

Функції `map`, `reduce` і `filter` дозволяють описувати обчислення декларативно, тобто визначати, що потрібно зробити, а не як саме це реалізовано. Завдяки цьому вони ідеально підходять для розподілених систем. Наприклад, у системах обробки великих даних, таких як Hadoop або Spark, функція `map` може застосовуватися незалежно до різних частин даних на різних вузлах, `filter` може прибирати зайві дані ще на початкових етапах обробки, а `reduce` об’єднувати часткові результати після виконання обчислень.

У наступному розділі ми розглянемо, як ці принципи використовуються в технології MapReduce, яка дозволяє масштабувати обчислення на тисячі серверів та ефективно працювати з великими обсягами даних.

У бібліотеці `multiprocessing` підтримка `map` реалізована через `Pool.map()`, що дозволяє легко розпаралелити виконання функції для кожного елемента списку. Проте `multiprocessing` **не має вбудованої підтримки `filter` та `reduce`**. Однак ці операції можна реалізувати вручну, використовуючи `Pool.map()` у поєднанні з додатковою логікою.

### **Паралельний filter з multiprocessing**

Оскільки `filter` застосовується до кожного елемента колекції незалежно, його можна реалізувати через `map`, а потім видалити `None` або `False` значення.

```python
from multiprocessing import Pool

numbers = [1, 2, 3, 4, 5, 6, 7, 8]

def is_even(x):
    return x if x % 2 == 0 else None  # Повертаємо None для непарних чисел

with Pool() as pool:
    filtered_results = pool.map(is_even, numbers)

even_numbers = [x for x in filtered_results if x is not None]

print(even_numbers)  # Виведе: [2, 4, 6, 8]
```

Тут ми використовуємо `map`, щоб перевірити кожен елемент списку, а потім вручну фільтруємо отримані значення.

---

### **Паралельний reduce з multiprocessing**

Оскільки `reduce` передбачає поступове агрегування значень, його можна реалізувати через `Pool.map()`, розбивши дані на блоки, а потім об’єднавши часткові результати.

```python
from multiprocessing import Pool
from functools import reduce

numbers = [1, 2, 3, 4, 5, 6, 7, 8]

def chunk_sum(sublist):
    return sum(sublist)

def parallel_reduce(func, data, chunks=4):
    chunk_size = len(data) // chunks
    chunks_list = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(chunks)]

    with Pool() as pool:
        partial_sums = pool.map(func, chunks_list)

    return reduce(func, partial_sums)

total_sum = parallel_reduce(sum, numbers, chunks=4)
print(total_sum)  # Виведе: 36
```

У цьому прикладі ми розбиваємо вхідний список на кілька частин, паралельно підраховуємо суму кожного блоку, а потім використовуємо `reduce` для остаточного об’єднання результатів.
### **4. Вступ до MapReduce**

Тепер, коли ми розібрали основи функціонального програмування та як `map`, `reduce` і `filter` працюють у контексті паралельних обчислень, настав час перейти до **MapReduce**. Це технологія, яка дозволяє обробляти великі обсяги даних у розподілених системах, ефективно використовуючи концепції, які ми вже розглянули.

#### **Що таке MapReduce?**

MapReduce — це програмна модель для паралельної обробки великих обсягів даних, що розроблена компанією Google і використовується в розподілених системах. Вона заснована на двох основних операціях:

- **Map** — розбиття вхідних даних на незалежні частини, які можна обробляти паралельно.
- **Reduce** — агрегування результатів після обробки.

Ця модель є надзвичайно ефективною, оскільки дозволяє виконувати обчислення на тисячах вузлів одночасно. Вона активно використовується у великих системах, таких як **Hadoop**, **Apache Spark** і **Google BigQuery**.

#### **Як працює MapReduce?**

MapReduce складається з кількох ключових етапів:

1. **Фаза Map**  
    Вхідні дані розбиваються на частини (шматки) та обробляються незалежними екземплярами функції `map`. Кожен вузол виконує свою частину роботи, а вихідними даними є проміжні результати у вигляді пар **(ключ, значення)**. Наприклад, для задачі підрахунку слів у тексті результатом цього етапу будуть пари `("слово", 1)`, які вказують, що дане слово зустрілося один раз.
    
2. **Фаза Shuffle (Перемішування)**  
    Після того, як усі вузли виконали свою частину роботи, необхідно згрупувати однакові ключі разом. Цей процес називається "перемішуванням" (`shuffle`) і виконується автоматично системою. У результаті всі значення для одного ключа опиняються в одному місці.
    
3. **Фаза Reduce**  
    Після групування даних застосовується функція `reduce`, яка агрегує всі значення, пов’язані з одним ключем. У випадку підрахунку слів вона підсумовує всі одиниці, отримуючи загальну кількість появ кожного слова.
    

#### **Чому MapReduce працює добре у кластерних середовищах?**

Основна причина успішності MapReduce у тому, що воно мінімізує взаємодію між вузлами. Оскільки етап `map` не потребує координації між процесами, обчислення можуть відбуватися незалежно, що дозволяє ефективно масштабувати систему. Крім того, система може автоматично обробляти збої: якщо один з вузлів виходить з ладу, його завдання може бути перерозподілено на інший вузол.

Розглянемо загальну схему виконання MapReduce на реальному прикладі у наступному розділі.


### **5. Реалізація простого прикладу MapReduce 

Щоб краще зрозуміти, як працює MapReduce, розглянемо класичний приклад **підрахунку слів у тексті**. Це стандартна задача, яка ілюструє принцип роботи цієї моделі та використання основних етапів `Map`, `Shuffle` і `Reduce`.

#### **Опис задачі**

Припустимо, у нас є текстовий файл, у якому потрібно підрахувати кількість повторень кожного слова. Наприклад, якщо текст виглядає так:

```
hello world hello
```

то очікуваний результат після обробки повинен бути:

```
hello 2
world 1
```

#### **Реалізація MapReduce у Python без бібліотек**

Для початку реалізуємо цю задачу без використання спеціалізованих бібліотек, щоб зрозуміти, як працюють основні принципи.

##### **Етап 1: Фаза Map**

На цьому етапі ми розбиваємо текст на слова та створюємо проміжний список пар `(слово, 1)`.

```python
def map_function(text):
    words = text.split()
    return [(word, 1) for word in words]

text = "hello world hello"
mapped = map_function(text)

print(mapped)
# Виведе: [('hello', 1), ('world', 1), ('hello', 1)]
```

##### **Етап 2: Фаза Shuffle**

Після отримання проміжних пар необхідно згрупувати однакові ключі разом.

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(mapped)
print(shuffled)
# Виведе: {'hello': [1, 1], 'world': [1]}
```

##### **Етап 3: Фаза Reduce**

На цьому етапі ми обчислюємо фінальні значення для кожного ключа.

```python
def reduce_function(shuffled_data):
    return {key: sum(values) for key, values in shuffled_data.items()}

reduced = reduce_function(shuffled)
print(reduced)
# Виведе: {'hello': 2, 'world': 1}
```

Ця реалізація добре ілюструє принципи роботи MapReduce:

- **Функція `map_function`** розбиває текст на слова і додає `1` до кожного.
- **Функція `shuffle`** групує однакові ключі разом.
- **Функція `reduce_function`** підсумовує значення для кожного слова.

---

#### **Реалізація MapReduce з використанням multiprocessing**

Якщо текст дуже великий, його можна обробляти паралельно. Використаємо `multiprocessing` для розпаралелювання етапу `map`.

```python
from multiprocessing import Pool
from functools import reduce

text_data = [
    "hello world hello",
    "map reduce example",
    "reduce map example example"
]

def map_function(text):
    words = text.split()
    return [(word, 1) for word in words]

with Pool() as pool:
    mapped_results = pool.map(map_function, text_data)

# Об’єднуємо всі результати з різних частин тексту
flat_mapped = [pair for sublist in mapped_results for pair in sublist]

shuffled = shuffle(flat_mapped)
reduced = reduce_function(shuffled)

print(reduced)
# Виведе: {'hello': 2, 'world': 1, 'map': 2, 'reduce': 2, 'example': 3}
```

У цьому коді:

- `Pool.map()` розпаралелює обробку текстових фрагментів.
- `flat_mapped` об’єднує результати всіх процесів.
- `shuffle` та `reduce_function` виконують групування та підсумовування.

---


### Приклад примітивної LLM
Можна реалізувати **примітивну мовну модель (LLM)**, використовуючи концепції `map`, `reduce` та `filter` у стилі MapReduce. Найпростіший варіант — це **передбачення наступного слова на основі частотного аналізу**.

### **Ідея моделі**

1. **Вхідні дані** – великий корпус текстів.
2. **Фаза Map** – розбиваємо текст на біграми (пари слів) у вигляді `(попереднє_слово, наступне_слово) → 1`.
3. **Фаза Shuffle** – групуємо всі однакові біграми.
4. **Фаза Reduce** – підраховуємо частоти появи наступного слова для кожного попереднього.
5. **Прогнозування** – на основі частотного аналізу вибираємо найімовірніше наступне слово.

---

### **Реалізація MapReduce для простої мовної моделі**

Спершу підготуємо корпус тексту:

```python
text_data = [
    "the quick brown fox jumps over the lazy dog",
    "the quick brown fox is fast",
    "the fox is very quick and smart"
]
```

#### **Етап 1: Map – Створення біграм**

Розбиваємо текст на пари слів у вигляді `("the", "quick")`, `("quick", "brown")` тощо:

```python
def map_function(text):
    words = text.split()
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return [(bigram, 1) for bigram in bigrams]

mapped_results = []
for line in text_data:
    mapped_results.extend(map_function(line))

print(mapped_results)
# Виведе список пар: [(('the', 'quick'), 1), (('quick', 'brown'), 1), ...]
```

#### **Етап 2: Shuffle – Групування біграм**

Агрегуємо всі однакові пари разом:

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(mapped_results)
print(shuffled)
# Виведе: {('the', 'quick'): [1, 1], ('quick', 'brown'): [1, 1], ...}
```

#### **Етап 3: Reduce – Підрахунок частот**

Підсумовуємо кількість появ кожної біграми:

```python
def reduce_function(shuffled_data):
    return {key: sum(values) for key, values in shuffled_data.items()}

reduced = reduce_function(shuffled)
print(reduced)
# Виведе: {('the', 'quick'): 2, ('quick', 'brown'): 2, ('brown', 'fox'): 2, ...}
```

#### **Етап 4: Прогнозування наступного слова**

Створюємо функцію, яка на основі цього аналізу буде пропонувати найімовірніше слово.

```python
def predict_next_word(word, model):
    candidates = {key[1]: count for key, count in model.items() if key[0] == word}
    if not candidates:
        return None
    return max(candidates, key=candidates.get)  # Вибираємо слово з найбільшою частотою

print(predict_next_word("the", reduced))  # Очікуваний результат: "quick"
print(predict_next_word("quick", reduced))  # Очікуваний результат: "brown"
print(predict_next_word("fox", reduced))  # Очікуваний результат: "is" або "jumps"
```

---

Ця модель працює за дуже спрощеним принципом: вона аналізує частотність пар слів і прогнозує наступне слово, виходячи з історичних частот. У реальних великих LLM використовується набагато складніший підхід: наприклад, **n-грамні моделі**, **word embeddings** (як Word2Vec), **трансформери**, які враховують контекст набагато ширше.

Проте цей підхід наочно демонструє, що MapReduce можна застосовувати для мовних задач, і, використовуючи розподілені обчислення, можна ефективно працювати з дуже великими корпусами текстів.

Наступний крок — якщо хочемо масштабувати це на великі набори даних, можна запаралелити обробку тексту за допомогою `multiprocessing`, як ми робили раніше з `map`.


Реалізуємо **розподілену обробку корпусу текстів**, уявивши, що у нас є **чотири книги у вигляді текстових файлів**. Використаємо **multiprocessing** для обробки даних у паралельному режимі.

---

### **Ідея підходу**

1. **Читаємо текстові файли паралельно**  
    Розбиваємо кожну книгу на біграми (пари слів).
2. **Фаза Map (паралельна)**  
    Використовуємо `Pool.map()`, щоб кожен процес обробляв свою частину тексту.
3. **Фаза Shuffle**  
    Групуємо біграми, щоб всі однакові ключі виявилися в одному місці.
4. **Фаза Reduce (паралельна)**  
    Використовуємо `Pool.map()`, щоб кожен процес підсумовував частоти біграм.
5. **Прогнозування наступного слова**  
    Використовуємо частотний аналіз для вибору найбільш імовірного наступного слова.

---

### **Реалізація паралельної обробки корпусу текстів**

#### **1. Функція читання тексту**

Читаємо вміст кожного файлу:

```python
def read_file(filename):
    with open(filename, "r") as f:
        return f.read()
```

---

#### **2. Паралельна фаза Map**

Кожен процес бере свій шматок тексту, розбиває його на слова та створює біграми:

```python
def map_function(text):
    words = text.lower().split()  # Зменшуємо регістр і розбиваємо на слова
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return [(bigram, 1) for bigram in bigrams]
```

Обробимо всі файли **паралельно**:

```python
from multiprocessing import Pool

file_list = [f"book_{i+1}.txt" for i in range(4)]  # 4 книги

with Pool() as pool:
    texts = pool.map(read_file, file_list)  # Читаємо всі книги
    mapped_results = pool.map(map_function, texts)  # Паралельна обробка тексту
```

Об’єднуємо всі біграми з різних процесів в один список:

```python
flat_mapped = [pair for sublist in mapped_results for pair in sublist]
```

---

#### **3. Фаза Shuffle**

Групуємо всі біграми разом:

```python
from collections import defaultdict

def shuffle(mapped_data):
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

shuffled = shuffle(flat_mapped)
```

---

#### **4. Паралельна фаза Reduce**

Тепер запустимо зведення частот **паралельно**:

```python
def reduce_function(key_values):
    key, values = key_values
    return (key, sum(values))

with Pool() as pool:
    reduced_results = pool.map(reduce_function, shuffled.items())

# Перетворюємо список у словник
final_model = dict(reduced_results)
```

---

#### **5. Прогнозування наступного слова**

Створюємо функцію, яка пропонує найбільш вживане наступне слово:

```python
def predict_next_word(word, model):
    candidates = {key[1]: count for key, count in model.items() if key[0] == word}
    if not candidates:
        return None
    return max(candidates, key=candidates.get)  # Найчастіше вживане слово

# Тестуємо прогнозування
print(predict_next_word("the", final_model))  # Очікуваний результат: "fox" або "journey"
print(predict_next_word("knowledge", final_model))  # Очікуваний результат: "is"
print(predict_next_word("to", final_model))  # Очікуваний результат: "be"
```

---

### **Висновки**

1. Ми створили **паралельну реалізацію MapReduce** для побудови **мовної моделі**, яка аналізує корпус текстів та передбачає наступне слово на основі частотного аналізу.
2. **Map (паралельний)** — кожен процес обробляє свою частину тексту, створюючи біграми.
3. **Shuffle (групування)** — всі однакові біграми об’єднуються.
4. **Reduce (паралельний)** — частоти біграм підсумовуються у декількох процесах.
5. **Прогнозування** — використовуємо найчастішу біграму для вибору наступного слова.

Цей підхід можна масштабувати на **великі текстові корпуси** та **кластерні системи**, де окремі вузли виконують `map`, а потім об’єднують результати через `reduce`.

фінальна версія коду **одним файлом**, яка реалізує **паралельну обробку текстових файлів** та **просту мовну модель на основі біграм** за допомогою `multiprocessing`.

```python
import os
from multiprocessing import Pool
from collections import defaultdict

# ---------------------- 1. Створення тестових файлів (емуляція книг) ----------------------
texts = [
    """the quick brown fox jumps over the lazy dog. the fox is fast and clever.""",
    """a journey of a thousand miles begins with a single step. the journey is long but rewarding.""",
    """to be or not to be, that is the question. the answer depends on the observer.""",
    """knowledge is power. the pursuit of knowledge shapes the mind and builds the future."""
]

file_list = [f"book_{i+1}.txt" for i in range(len(texts))]

def create_test_files():
    """Створює тестові текстові файли, якщо вони ще не існують."""
    for i, text in enumerate(texts):
        if not os.path.exists(file_list[i]):
            with open(file_list[i], "w", encoding="utf-8") as f:
                f.write(text)

# ---------------------- 2. Читання файлів ----------------------
def read_file(filename):
    """Зчитує вміст текстового файлу."""
    with open(filename, "r", encoding="utf-8") as f:
        return f.read().lower()  # Приводимо текст до нижнього регістру

# ---------------------- 3. Фаза Map: Розбиття тексту на біграми ----------------------
def map_function(text):
    """Розбиває текст на біграми (пари слів)."""
    words = text.split()
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return [(bigram, 1) for bigram in bigrams]

# ---------------------- 4. Фаза Shuffle: Групування біграм ----------------------
def shuffle(mapped_data):
    """Групує біграми, щоб об’єднати однакові ключі."""
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

# ---------------------- 5. Фаза Reduce: Підрахунок частот ----------------------
def reduce_function(key_values):
    """Обчислює суму частот для кожної біграми."""
    key, values = key_values
    return (key, sum(values))

# ---------------------- 6. Функція прогнозування наступного слова ----------------------
def predict_next_word(word, model):
    """Прогнозує наступне слово на основі частотного аналізу біграм."""
    candidates = {key[1]: count for key, count in model.items() if key[0] == word}
    if not candidates:
        return None
    return max(candidates, key=candidates.get)  # Найбільш вживане слово

# ---------------------- Головна функція ----------------------
if __name__ == "__main__":
    create_test_files()  # Створюємо тестові файли, якщо їх немає

    with Pool() as pool:
        texts = pool.map(read_file, file_list)  # Паралельне читання файлів
        mapped_results = pool.map(map_function, texts)  # Паралельна обробка тексту

    # Об’єднуємо всі біграми з різних процесів в один список
    flat_mapped = [pair for sublist in mapped_results for pair in sublist]

    # Фаза Shuffle: групуємо біграми
    shuffled = shuffle(flat_mapped)

    # Фаза Reduce: обчислюємо частоти біграм паралельно
    with Pool() as pool:
        reduced_results = pool.map(reduce_function, shuffled.items())

    # Перетворюємо список у словник
    final_model = dict(reduced_results)

    # ---------------------- CLI для користувача ----------------------
    print("📖 Simple CLI Chatbot using MapReduce")
    print("Type a word and get the most probable next word (type 'exit' to quit).")

    while True:
        user_input = input("\nEnter a word: ").strip().lower()
        if user_input == "exit":
            print("Goodbye! 👋")
            break

        prediction = predict_next_word(user_input, final_model)
        if prediction:
            print(f"🤖 Predicted next word: {prediction}")
        else:
            print("❌ No prediction available for this word.")

```


## Приклад з n-грамами та вікіпедією
```
pip install wikipedia-api
```


``` python
import os
import wikipediaapi
from multiprocessing import Pool
from collections import defaultdict
import re

# ---------------------- 1. Завантаження статей з Вікіпедії ----------------------
wiki = wikipediaapi.Wikipedia(
    language="uk",
    user_agent="MyWikipediaBot/1.0 (contact: myemail@example.com)"
)

articles = [
    "Математика",
    "Комп'ютерні науки",
    "Обчислювальна техніка",
    "Штучний інтелект"
]

def fetch_wikipedia_article(title):
    """Отримує текст статті з Вікіпедії"""
    page = wiki.page(title)
    if not page.exists():
        return ""
    text = page.text
    text = re.sub(r'\n+', ' ', text)  # Видаляємо зайві переноси рядків
    text = re.sub(r'[^а-яА-ЯіІїЇєЄґҐa-zA-Z0-9\s]', '', text)  # Видаляємо знаки пунктуації
    return text.lower()

def create_corpus_from_wikipedia():
    """Завантажує статті та створює текстовий корпус"""
    with Pool() as pool:
        texts = pool.map(fetch_wikipedia_article, articles)
    
    corpus_text = " ".join(texts).strip()

    if not corpus_text:
        print("⚠️ ПОМИЛКА: Отримано порожній корпус! Переконайтеся, що статті існують.")
        return False

    with open("corpus.txt", "w", encoding="utf-8") as f:
        f.write(corpus_text)
    
    print(f"✅ Корпус успішно створено! Довжина: {len(corpus_text)} символів.")
    return True

# ---------------------- 2. Читання корпусу ----------------------
def read_corpus():
    """Читає текстовий корпус"""
    if not os.path.exists("corpus.txt"):
        print("⚠️ ПОМИЛКА: Файл corpus.txt не знайдено!")
        return ""
    
    with open("corpus.txt", "r", encoding="utf-8") as f:
        return f.read().strip()

# ---------------------- 3. Фаза Map: Генерація n-грам ----------------------
def generate_ngrams(text, n=2):
    """Генерує n-граму (за замовчуванням біграми)"""
    words = text.split()
    if len(words) < n:
        return []
    ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]
    return [(ngram, 1) for ngram in ngrams]

def generate_ngrams_wrapper(args):
    """Обгортка для multiprocessing"""
    text, n = args
    return generate_ngrams(text, n)

# ---------------------- 4. Фаза Shuffle: Групування n-грам ----------------------
def shuffle(mapped_data):
    """Групує n-граму за ключем"""
    grouped_data = defaultdict(list)
    for key, value in mapped_data:
        grouped_data[key].append(value)
    return grouped_data

# ---------------------- 5. Фаза Reduce: Підрахунок частот n-грам ----------------------
def reduce_function(key_values):
    """Обчислює суму частот для кожної n-грами"""
    key, values = key_values
    return (key, sum(values))

# ---------------------- 6. Прогнозування наступного слова ----------------------
def predict_next_word(context, model, n):
    """Прогнозує наступне слово на основі n-грам"""
    words = context.split()
    if len(words) < n - 1:
        print("❌ Введено занадто мало слів для прогнозу!")
        return None

    context_tuple = tuple(words[-(n - 1):])  # Беремо останні n-1 слів
    candidates = {key[-1]: count for key, count in model.items() if key[:-1] == context_tuple}

    if not candidates:
        print(f"❌ Немає прогнозу для контексту: {context_tuple}")
        return None

    return max(candidates, key=candidates.get)  # Найімовірніше слово

# ---------------------- 7. Головна функція ----------------------
if __name__ == "__main__":
    print("📥 Завантаження статей з Вікіпедії...")
    if not create_corpus_from_wikipedia():
        exit(1)

    print("📖 Читання корпусу...")
    text = read_corpus()
    if not text:
        print("⚠️ ПОМИЛКА: Корпус порожній! Завантаження не вдалося.")
        exit(1)

    n = int(input("🔢 Введіть значення n для n-грам (наприклад, 2 для біграм, 3 для триграм): ").strip())

    print("🔨 Обробка тексту...")
    with Pool() as pool:
        mapped_results = pool.map(generate_ngrams_wrapper, [(text, n)])

    flat_mapped = [pair for sublist in mapped_results for pair in sublist]

    if not flat_mapped:
        print("⚠️ ПОМИЛКА: Не знайдено жодної n-грамної пари. Можливо, текст занадто короткий?")
        exit(1)

    print("🔄 Групування n-грам...")
    shuffled = shuffle(flat_mapped)

    print("🧮 Підрахунок частот n-грам...")
    with Pool() as pool:
        reduced_results = pool.map(reduce_function, shuffled.items())

    final_model = dict(reduced_results)

    if not final_model:
        print("⚠️ ПОМИЛКА: Модель порожня! Не вдалося створити n-грамну статистику.")
        exit(1)

    # ---------------------- CLI ----------------------
    print("\n🤖 Simple CLI Chatbot using n-gram Model")
    print("Введіть кілька слів, і бот спрогнозує наступне слово (напишіть 'exit' для виходу).")

    while True:
        user_input = input("\nВведіть слова: ").strip().lower()
        if user_input == "exit":
            print("До побачення! 👋")
            break

        prediction = predict_next_word(user_input, final_model, n)
        if prediction:
            print(f"🤖 Прогнозоване слово: {prediction}")
        else:
            print("❌ Немає прогнозу для цього контексту.")

```

